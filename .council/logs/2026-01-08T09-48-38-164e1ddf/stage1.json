{
  "responses": [
    {
      "model": "google/gemini-3-pro-preview",
      "response": "Here is the code review for commit `78a8f54`.\n\n### 1. Summary\n\nThis commit implements the Phase 2 \"Memory Blocks\" architecture for context engineering. It introduces:\n*   A **Block Assembler** (`assembler.py`) to construct prompts from System, Project, Task, History, and Knowledge blocks, managing token budgets and priorities.\n*   A **History Compressor** (`compressor.py`) to summarize conversation history within token limits.\n*   **Provenance** tracking structures (`schemas.py` and `provenance/service.py`) to attribute memory sources.\n*   Updates to the **Ranker** (`ranker.py`) to calculate retrieval scores and attach provenance to retrieved memories.\n*   **Token Efficiency** metrics (`evaluation/token_efficiency.py`).\n\n### 2. Quality Assessment\n\n*   **Readability**: The code is well-structured and follows Python standards. Type hinting is pervasive and consistent. Docstrings clearly reference specific ADRs and Issues.\n*   **Modularity**: Responsibilities are clearly separated (Assembler vs. Compressor vs. Ranker). The schema definitions are clean.\n*   **Maintainability**: Constants like `DEFAULT_TOKEN_BUDGETS` and `XML_OVERHEAD_PER_BLOCK` are well-defined, making future tuning easier.\n\n### 3. Potential Issues\n\nI have identified correctness concerns regarding data integrity and schema compatibility.\n\n#### A. Provenance Overwriting in Ranker (Logic/Correctness)\n**File**: `src/memory/retrieval/ranker.py`\n**Line**: ~187 (`rank_with_provenance` method)\n\nWhen `rank_with_provenance` is called, it constructs a *new* `Provenance` object representing the retrieval event and attaches it to the memory:\n\n```python\nprovenance = Provenance(\n    source_id=memory.metadata.get(\"memory_id\", \"unknown\"),\n    source_type=\"memory\",\n    # ...\n)\nmemory_with_provenance = memory.model_copy(\n    update={\"provenance\": provenance}\n)\n```\n\n**Issue**: If the stored `Memory` object already contains provenance data (e.g., source file path, original author) from its ingestion time, this operation **overwrite** that data with generic \"source_type: memory\" provenance. This defeats the purpose of provenance by losing the original source verification.\n\n#### B. Schema Compatibility (Runtime Correctness)\n**File**: `src/memory/retrieval/ranker.py`\n**Line**: 193\n\n```python\nmemory_with_provenance = memory.model_copy(\n    update={\"provenance\": provenance}\n)\n```\n\n**Issue**: The code assumes the `Memory` model (imported from `src.memory.schemas`, which is not part of this commit) has a `provenance` field.\n1.  If the existing `Memory` Pydantic model does not have this field defined, `model_copy(update=...)` will likely raise a `ValidationError` (extra fields not permitted) or fail silently depending on configuration.\n2.  `Provenance` is defined as a standard `@dataclass` in `blocks/schemas.py`, while `Memory` is likely a Pydantic `BaseModel`. While Pydantic V2 handles dataclasses well, this interaction needs verification to ensure serialization runs correctly without validation errors.\n\n#### C. Token Counting Consistency\n**File**: `src/memory/blocks/assembler.py` vs `src/memory/blocks/compressor.py`\n\nThe assembler relies on `hist_compressor.count_tokens` for budget enforcement.\n```python\n# src/memory/blocks/compressor.py\ndef count_tokens(self, text: str) -> int:\n    # ...\n    words = text.split()\n    return int(len(words) * 1.3)\n```\n\n**Issue**: This `split()` based approach underestimates token counts for strings without spaces (e.g., long URLs, code snippets without spaces, base64 strings).\nWhile `_truncate_preserving_format` in the Assembler has a fallback for character limits (`words` list is effectively length 1, but chars are high), the initial `if token_count > budget` check might pass erroneously. If a block contains 5000 chars of contiguous text, `count_tokens` sees ~1.3 tokens, the check `1.3 > budget` fails, and the massive block is inserted, potentially overflowing the actual LLM context window.\n\n### 4. Recommendations\n\n1.  **Fix Provenance Overwriting**:\n    In `ranker.py`, check if `memory.provenance` already exists. If it does, either list the new retrieval provenance as a separate metadata field (e.g., `retrieval_context`) or retain the original provenance. Do not overwrite the origin story with the retrieval event.\n\n2.  **Verify Memory Schema**:\n    Ensure `src.memory.schemas.Memory` has been updated to include:\n    ```python\n    provenance: Optional[Provenance] = None\n    ```\n    If `Memory` is a Pydantic model, ensure it is configured to accept the `Provenance` dataclass (`arbitrary_types_allowed=True` or standard V2 dataclass support).\n\n3.  **Improve Assembler Truncation Logic**:\n    In `src/memory/blocks/assembler.py`, the `_truncate_preserving_format` method:\n    ```python\n    # Fallback: word truncation if even first line is too long\n    words = content.split()\n    ```\n    Should also verify character length limits if `words` is small but `len(content)` is large, to prevent the `split()` token counting edge case.\n\n### 5. Verdict\n\n**NEEDS REVIEW**\n\nThe code logic for assembly is sound, but the `ranker.py` implementation poses a risk of data loss (overwriting original provenance) and potential runtime errors (schema incompatibility). These should be verified and fixed before merging."
    },
    {
      "model": "anthropic/claude-opus-4.5",
      "response": "# Code Review for Memory Blocks Architecture (ADR-003 Phase 2)\n\n## Summary\n\nThis code implements a Memory Blocks architecture for structured context management in an LLM-based system. It consists of:\n\n1. **Block Assembler** (`assembler.py`): Assembles context from 5 block types (System, Project, Task, History, Knowledge) within token budgets\n2. **History Compressor** (`compressor.py`): Compresses conversation history to achieve token efficiency\n3. **Schemas** (`schemas.py`): Data structures for BlockType, Provenance, and MemoryBlock\n4. **Provenance Service** (`provenance/service.py`): Tracks source attribution for retrieved content\n5. **Token Efficiency Metric** (`evaluation/token_efficiency.py`): Measures token efficiency against targets\n6. **Memory Ranker** (`retrieval/ranker.py`): Ranks memories by relevance with provenance support\n\n## Quality Assessment\n\n**Strengths:**\n- Well-documented code with clear docstrings and ADR references\n- Good separation of concerns across modules\n- Comprehensive type hints throughout\n- Proper use of dataclasses and enums\n- XML escaping for security against prompt injection\n\n**Areas of Concern:**\n- Several correctness issues identified below\n- Some inconsistencies in token counting methodology\n- Missing validation in critical paths\n\n## Potential Issues\n\n### Critical Issues\n\n#### 1. Memory Model Missing `provenance` Field (`ranker.py:186-190`)\n```python\nmemory_with_provenance = memory.model_copy(\n    update={\"provenance\": provenance}\n)\n```\nThe `Memory` model (imported from `src.memory.schemas`) likely doesn't have a `provenance` field. This will fail at runtime with a validation error or silently ignore the update.\n\n**Impact**: Runtime failure when `rank_with_provenance` is called with `attach_provenance=True`\n\n#### 2. Token Efficiency Metric Logic Inverted (`evaluation/token_efficiency.py:52-56`)\n```python\nefficiency_improvement = 1 - (total_tokens / self.baseline)\n```\nIf `total_tokens` is **less** than baseline (which is the goal), this gives a positive improvement. However, the metric is measuring \"how much we saved\" when it should measure \"how efficiently we're using tokens\". The current logic means:\n- If `total_tokens = 3500` and `baseline = 5000`, efficiency = 0.30 (30%)\n- But this assumes the compression always reduces tokens, which may not be the intent\n\nThe naming and logic need clarification - is baseline the \"uncompressed\" size or the \"budget\"?\n\n#### 3. XML Escaping Not Applied to Block Type in `to_prompt` (`assembler.py:337-343`)\n```python\ndef to_prompt(self, blocks: list[MemoryBlock]) -> str:\n    # ... escaped_content uses _escape_xml_content\n    # But block.block_type.value is used directly\n```\nWhile content is escaped, the block type value comes from an enum and is safe. However, the `to_prompt` method is incomplete (truncated), making full review impossible.\n\n### Moderate Issues\n\n#### 4. Inconsistent Token Counting (`compressor.py:43-50` vs `assembler.py:72-82`)\n```python\n# compressor.py\nreturn int(len(words) * 1.3)\n\n# assembler.py uses compressor's method\nreturn self._compressor.count_tokens(text)\n```\nThe token estimation is consistent within the codebase, but the approximation `len(words) * 1.3` is crude. Different content types (code vs prose) have very different token densities, which could cause budget overruns.\n\n#### 5. Missing Budget Validation for Project Block (`assembler.py:113-127`)\n```python\nasync def build_project_block(self, user_id: str) -> MemoryBlock:\n    content = f\"Project context for user {user_id}.\"\n    token_count = self._count_tokens(content)\n    # No budget check or truncation like other blocks\n    return MemoryBlock(...)\n```\nUnlike `build_system_block` and `build_task_block`, the project block doesn't enforce its budget.\n\n#### 6. Race Condition in Provenance Service (`provenance/service.py`)\nThe in-memory stores (`_provenance_store`, `_retrieval_history`) are not thread-safe:\n```python\nif memory_id not in self._retrieval_history:\n    self._retrieval_history[memory_id] = []\nself._retrieval_history[memory_id].append(event)\n```\nConcurrent async operations could lead to data corruption.\n\n#### 7. Empty String Query Creates Invalid Provenance (`assembler.py:198-210`)\n```python\nasync def build_knowledge_block(self, user_id: str, query: str) -> MemoryBlock:\n    if not query:\n        return MemoryBlock(\n            # ... no provenance\n        )\n    # ...\n    provenance = Provenance(\n        source_id=f\"knowledge:{user_id}:{query[:50]}\",  # Truncation could break ID uniqueness\n```\nThe `source_id` truncates the query to 50 chars, which could create duplicate IDs for different queries.\n\n### Minor Issues\n\n#### 8. Decay Half-Life Edge Case (`ranker.py:126-128`)\n```python\nif self.decay_half_life_days <= 0:\n    return 0.0\n```\nReturning 0.0 for invalid configuration silently degrades ranking. Should raise an error during `__init__`.\n\n#### 9. Inconsistent Empty Content Handling (`assembler.py:257-262`)\n```python\nif block.content:\n    block_cost += self.XML_OVERHEAD_PER_BLOCK\n```\nEmpty blocks don't get XML overhead, but `to_prompt` may still generate tags for them (can't verify due to truncation).\n\n#### 10. Hardcoded Truncation Marker (`assembler.py:298`)\n```python\ntruncation_marker = \"...\"\n```\nThe marker tokens are counted correctly, but this could conflict with actual content containing \"...\".\n\n## Recommendations\n\n1. **Fix the Memory model** to include an optional `provenance` field, or change the approach in `rank_with_provenance` to return provenance separately.\n\n2. **Clarify token efficiency semantics** - document whether baseline is \"uncompressed content size\" or \"token budget\", and ensure the metric matches its intended use.\n\n3. **Add budget enforcement to `build_project_block`** for consistency with other block builders.\n\n4. **Add thread-safety** to `ProvenanceService` using `asyncio.Lock` for the async methods.\n\n5. **Validate configuration** in `MemoryRanker.__init__` to reject invalid weights/half-life values.\n\n6. **Generate unique source_ids** using hashing rather than truncation:\n   ```python\n   import hashlib\n   source_id = f\"knowledge:{user_id}:{hashlib.sha256(query.encode()).hexdigest()[:16]}\"\n   ```\n\n7. **Consider using tiktoken** for accurate token counting in production, as noted in the comment.\n\n8. **Complete the `to_prompt` method review** - the file was truncated.\n\n## Verdict\n\n**NEEDS REVIEW**\n\nThe code has solid architecture and good documentation, but there are several correctness issues that need attention:\n\n1. The `Memory` model compatibility with provenance attachment is a potential runtime failure\n2. The token efficiency metric semantics need clarification\n3. Thread-safety concerns in the provenance service\n4. Inconsistent budget enforcement across block builders\n\nThese issues are not necessarily blockers but require human judgment on:\n- Whether the `Memory` schema already supports `provenance` (not visible in this review)\n- Whether the efficiency metric is being used correctly\n- Whether concurrency is a concern for the current deployment"
    },
    {
      "model": "x-ai/grok-4",
      "response": "### Code Review for Commit `78a8f54`\n\n#### 1. Summary\nThis commit implements the core components of a Memory Blocks architecture (as per ADR-003 Phase 2) for structured context management in an AI assistant. It defines five block types (System, Project, Task, History, Knowledge) and provides an assembler to build, prioritize, and trim these blocks to fit within token budgets. Additional features include history compression for token efficiency, provenance tracking for retrieved items, token efficiency metrics to verify a 30% improvement target, and memory ranking based on similarity, recency, and confidence. The code uses placeholders for external dependencies (e.g., actual data retrieval), focusing on the structural and logical framework. It ensures context is assembled into a prompt-friendly format while handling truncation and overhead (e.g., XML wrappers).\n\n#### 2. Quality Assessment\n- **Readability**: High. The code is well-organized with consistent naming conventions (e.g., snake_case for methods, clear enums like `BlockType`), comprehensive docstrings, and type hints. Modules are logically separated (e.g., schemas in `schemas.py`, assembly logic in `assembler.py`). Comments reference related issues and ADRs, aiding understanding. However, some methods (e.g., `build_project_block` in `src/memory/blocks/assembler.py`) use placeholders, which could be flagged more explicitly as TODOs.\n  \n- **Maintainability**: Strong. The design is modular, using dataclasses (e.g., `MemoryBlock`, `Provenance`) and enums for extensibility. Defaults (e.g., `DEFAULT_BLOCK_PRIORITIES` in `src/memory/blocks/schemas.py`) are centralized and overridable. In-memory storage (e.g., in `ProvenanceService` in `src/memory/provenance/service.py`) is simple but easy to replace with a database. Code reuse is evident (e.g., `_count_tokens` delegates to `HistoryCompressor`). Potential improvements include adding more unit tests (implied but not present) and abstracting token counting for easier replacement with precise tokenizers.\n\n- **Overall Code Quality**: Good, with a focus on ADR requirements (e.g., provenance for all retrieved items, 30% token efficiency). Approximations (e.g., word-based token counting in `src/memory/blocks/compressor.py:count_tokens`) are noted as placeholders for production (e.g., using tiktoken). No redundant code, and error handling is minimal but appropriate (e.g., handling empty inputs gracefully).\n\n#### 3. Potential Issues\nFocusing on correctness as per the review instructions, I've identified logical inconsistencies, incomplete implementations, and minor bugs. No critical security vulnerabilities or performance bottlenecks were found, but approximations could lead to inaccuracies in production.\n\n- **Correctness/Bugs**:\n  - **Inconsistent Compressor Usage**: In `src/memory/blocks/assembler.py:__init__`, `self._compressor` is initialized as `HistoryCompressor()` (default `max_tokens=1000`). However, in `build_history_block` (line ~170), a new `HistoryCompressor(max_tokens=budget)` is created for compression, but token counting falls back to `self._count_tokens` (which uses the default compressor). This could lead to minor discrepancies if compressor configurations differ in the future. (No immediate bug, but logical inconsistency.)\n  \n  - **Truncation Logic Inconsistencies**: In `src/memory/blocks/compressor.py:_truncate_to_tokens` (line ~180), truncation stops at line boundaries without adding a truncation marker (e.g., \"...\"), potentially leaving truncated content abrupt. In contrast, `src/memory/blocks/assembler.py:_truncate_preserving_format` (line ~300) adds \"...\" appropriately. This could result in incomplete or misleading history blocks if truncation occurs.\n  \n  - **Token Count Recalculation Missing in Some Paths**: In `src/memory/blocks/assembler.py:_trim_to_budget` (line ~250), after truncating content, `truncated_tokens` is recalculated correctly. However, in `src/memory/blocks/compressor.py:compress` (line ~80), if recent messages are truncated, there's no recalculation of `recent_tokens` post-truncation, which might overestimate remaining budget for summaries.\n  \n  - **Placeholder Logic May Not Handle Real Data Correctly**: In `src/memory/blocks/assembler.py:build_knowledge_block` (line ~200), provenance is created with placeholders (e.g., `retrieval_score=1.0`), but in production, this should integrate with actual retrieval systems. Similarly, `build_project_block` (line ~120) uses a static string, which won't reflect dynamic user data. This isn't a bug per se but could lead to incorrect context if not implemented.\n  \n  - **Provenance Attachment Assumes Pydantic**: In `src/memory/retrieval/ranker.py:rank_with_provenance` (line ~200), `memory.model_copy(update={\"provenance\": provenance})` assumes `Memory` is a Pydantic model. If it's not (schemas not fully provided), this will fail at runtime.\n  \n  - **Recency Calculation Edge Cases**: In `src/memory/retrieval/ranker.py:calculate_recency` (line ~100), if `last_accessed_at` is in the future (e.g., due to clock skew), `age_days` is clamped to 0, which is correct. However, if `decay_half_life_days <= 0`, it returns 0.0, but a guard prevents division by zero\u2014good, but could log a warning for invalid config.\n  \n  - **Efficiency Metric Assumptions**: In `src/memory/evaluation/token_efficiency.py:calculate_efficiency` (line ~40), efficiency is calculated as `1 - (total_tokens / baseline)`, but if `total_tokens > baseline`, this yields negative values (not handled; could add clamping to 0.0). Also, it doesn't account for XML overhead in the metric, though the assembler does.\n\n- **Security Vulnerabilities**:\n  - **XML Escaping Incomplete**: In `src/memory/blocks/assembler.py:_escape_xml_content` (line ~320, truncated in prompt), it escapes `&`, `<`, `>`, but not quotes (`\"` or `'`) or control characters. If content is user-generated and embedded in XML attributes, this could allow injection. (Low risk if only used in CDATA or text nodes.)\n  \n  - No input validation for user_id or query in methods like `build_knowledge_block`, but since it's internal, low risk.\n\n- **Performance Concerns**:\n  - **Approximate Token Counting**: Repeated calls to `count_tokens` (e.g., in loops during truncation in `src/memory/blocks/assembler.py:_truncate_preserving_format`) use a simple word-split approximation (1.3 tokens/word), which is inefficient and inaccurate for large texts or actual LLM tokenizers.\n  - **In-Memory Storage**: `ProvenanceService` in `src/memory/provenance/service.py` uses dicts, fine for small scale but will consume memory for large histories without persistence.\n  - **Ranking Scalability**: `rank` in `src/memory/retrieval/ranker.py` (line ~150) scores and sorts all memories in O(n log n), inefficient for thousands of memories without indexing.\n\n#### 4. Recommendations\n- **Improve Token Handling**: Replace approximate `count_tokens` with a precise tokenizer (e.g., integrate tiktoken) across all files (e.g., `src/memory/blocks/compressor.py:count_tokens`). This would ensure correctness in budget enforcement.\n  \n- **Enhance Truncation Consistency**: Standardize truncation by always adding a marker (e.g., \"... [truncated]\") and recalculating tokens post-truncation. Add this to `src/memory/blocks/compressor.py:_truncate_to_tokens`.\n  \n- **Complete Placeholders**: Implement real data fetching in `build_project_block` and `build_knowledge_block` (e.g., integrate with a database). For summarization in `src/memory/blocks/compressor.py:summarize`, use an LLM for better quality.\n  \n- **Add Error Handling and Validation**: In `src/memory/retrieval/ranker.py:calculate_recency`, add logging for invalid dates. Validate inputs like `user_id` to prevent empty or malicious strings.\n  \n- **Testing Suggestions**: Add unit tests for edge cases, e.g., budget overflows in `assemble`, zero-token budgets, and provenance serialization in `MemoryBlock.to_dict` (src/memory/blocks/schemas.py).\n  \n- **Security Enhancements**: Expand XML escaping to include quotes and control chars. Consider using a library like `xml.sax.saxutils.escape`.\n  \n- **Performance Optimizations**: For ranking, pre-filter memories using an index (e.g., vector embeddings) before scoring. Make `ProvenanceService` configurable for persistent storage.\n\n**Verdict: APPROVED**  \nThe code is logically correct, meets the ADR requirements, and has no critical bugs that prevent functionality. Placeholders and approximations are noted as such and don't impact core correctness. It's ready for production with the understanding that integrations (e.g., real data retrieval, precise tokenization) should be added soon."
    }
  ],
  "usage": {
    "prompt_tokens": 32131,
    "completion_tokens": 11095,
    "total_tokens": 43226
  },
  "timestamp": "2026-01-08T09:49:57.489090"
}